\documentclass[a4paper,10pt]{article}

\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}    
\usepackage{lmodern}
\usepackage[french]{babel}
\usepackage{hyperref} %créé hyperliens
\usepackage{fancyhdr}
\usepackage[left=2cm, right=2cm,foot=2cm]{geometry}
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}

\usepackage{placeins}


\pagestyle{fancy}
\setlength{\headheight}{13pt}
\renewcommand{\footrulewidth}{1pt}
\cfoot{Télécom ParisTech}
\rfoot{\thepage}
\lfoot{\includegraphics[scale=0.15]{./logo.png}}


%opening
\title{PAF - Contraste et catégorisation}
\author{Antoine Bellami\\Aurélien Blicq\\Clément Bonet\\Benoit Malézieux\\Louis Penet de Monterno\\Bastien Vagne}

\begin{document}

%\maketitle
\begin{titlepage}

	\centering
	\includegraphics[scale=0.75]{./logo.png}\par\vspace{1cm}
	{\scshape\LARGE Telecom ParisTech \par}
	\vspace{1cm}
	{\scshape\Large PAF\par}
	\vspace{1.5cm}
	{\huge\bfseries Contraste et Catégorisation\par}
	\vspace{2cm}
	{\Large\itshape Antoine Bellami\par}
    {\Large\itshape Aurélien Blicq\par}
    {\Large\itshape Clément Bonet\par}
    {\Large\itshape Benoit Malézieux\par}
    {\Large\itshape Louis Penet de Monterno\par}
    {\Large\itshape Bastien Vagne\par}

	\vfill

% Bottom of the page
	{\large \today\par}
\end{titlepage}


\newpage

\renewcommand{\contentsname}{Sommaire} 
\tableofcontents

\newpage


\section{Introduction}

Les techniques de clustering produisent des classes d’objets et des jugements d’appartenance à ces classes.
L’idée du contraste consiste à opérer une différence vectorielle entre un objet et la classe la plus proche, puis de caractériser cette différence. Ainsi, une tomate noire sera décrite comme telle parce que la différence avec le prototype "tomate" sera proche du prototype "noir".

\paragraph{}

L’opération de contraste permet à une IA :

\renewcommand{\labelitemi}{\textbullet}

\begin{itemize}
 \item de comprendre le sens de "petite bactérie" et de "petite galaxie" sans passer par l’ensemble des objets petits, car dans les deux cas "petit" correspond au contraste avec le prototype ;
 \item de produire des descriptions pertinentes : "c’est un chanteur qui a dix millions de vues sur Youtube", car c’est ce qui le distingue du prototype de chanteur ;
 \item de produire des négations et des explications : ce n’est pas un écrivain, parce qu’il n’a pas écrit de romans ;
 \item de détecter des anomalies : un chat qui parle ;
 \item d'apprendre à partir d’un seul exemple : un enfant qui voit pour la première fois un chat blanc, noir aux extrémités, comprend et mémorise l’expression "chat siamois."
\end{itemize}

\begin{figure}[h]
	\begin{center}
    \includegraphics[scale=0.5]{./schemaGeneral.png}
    \end{center}
    \caption{Exemple d'utilisation du contraste}
\end{figure}

\paragraph{}

Il s’agit de réaliser une première implémentation du contraste sur un jeu de données réel ou artificiel. L’algorithme sera appliqué deux fois : une fois sur les objets eux-mêmes, une deuxième fois sur les contrastes (différences objets-prototypes). La deuxième application du contraste est censée faire ressortir des modifieurs interprétables comme des adjectifs.
La suite du projet consiste à placer le contraste au coeur de l’activité d’apprentissage, en l’utilisant pour former les catégories. L’idée est de permettre un apprentissage pertinent en présence de (très) peu de données.

\paragraph{}

Ce problème pourrait avoir plusieurs applications. Prenons l’exemple d’un système de machine learning devant reconnaître des images. Ce système, pour pouvoir fonctionner correctement, nécessite des quantités astronomiques de données, parfois difficiles à collecter. Les techniques de machine learning sont actuellement extrêmement désemparées lorsqu’elles ont affaire à peu de données. L’apprentissage incrémental permettrait de reconnaître des motifs en “one-shot” , c’est-à-dire avec une ou quelques données d’apprentissage. Notre objectif est d’implémenter ce type d’apprentissage en utilisant le concept de contraste afin de construire les clusters.
Une autre application du contraste serait le repérage d’exceptions.


\section{Méthode}

On travaille dans un espace dont la dimension est le nombre d’attributs qui nous intéressent (la taille, la forme et le goût de fruits par exemple).

On a commencé par effectuer une classification grâce à l’algorithme de k-means. Il existe différentes possibilités pour choisir le k, c’est-à-dire le bon nombre de catégories : on peut notamment utiliser une métrique particulière pour comparer les résultats de l’algorithme avec différentes valeurs. Le problème est qu'il n'existe pas un nombre parfait de clustering \cite{ref2}, il faut donc essayer de choisir ce nombre de manière la plus optimale possible.

\begin{flushleft}
Un bon schéma vaut 1000 mots pour décrire l’algorithme de k-means :
\end{flushleft}

\begin{figure}[h]
 \begin{center}
  \includegraphics[scale=0.1]{./k-means.png}
 \end{center}
 \caption{Illustration du déroulement de l'algorithme des k-means - Mquantin (Wikipedia)}
\end{figure}

Un cluster est défini par un centre et un ensemble de points (vecteur centre et vecteur écarts-types).
Pour une meilleure classification, on peut décider de normaliser par rapport aux écarts-types sur chacune des dimensions (grâce à la matrice de covariance).

Pour contraster chaque cluster, on calcule la différence entre chaque point et le centre afin de se ramener autour de l’origine. On catégorise donc les différences en déterminant la proximité de chaque point avec le centre (les points éloignés du centre ont un contraste élevé avec la moyenne des points, ils auront des caractéristiques particulières par rapport à leur catégorie).

On commence par travailler sur un jeu de données simple (qui comporte au moins une dizaine de dimensions) et que l’on maîtrise bien : les fruits. Après avoir rentré à la main quelques données de fruits, on construit la base de données en bruitant les différentes caractéristiques de manière anisotrope : on ne bruite pas tout en même temps de la même manière pour espérer avoir des groupes de contraste interprétables.

En parallèle, on essaye de créer un jeu de données plus conséquent avec plus de dimension. Pour cela, on utilise une base de données de livres open-source en français dans lesquels on va extraire de manière pertinente les mots les plus fréquents grâce à l'algorithme TF-IDF \cite{ref1}. Ces mots correspondront à nos dimensions.


\section{Premier programme}

\subsection{Algorithme k-means}

On utilise l’implémentation de k-means fournie par le module python sklearn.
La difficulté d’application de l’algorithme consiste à trouver le nombre optimal de clusters, c’est-à-dire le nombre de classes dans lequel on va partager notre ensemble de données. 

Pour cela, on a commencé par appliquer la méthode du “coude” (elbow method) qui consiste à appliquer l’algorithme pour plusieurs k différents et à tracer les sommes des variances de la distance euclidienne de chaque cluster par rapport à leurs centres. On repère ensuite la valeur de k “qui se situe dans le coude”. Cette valeur de k sera le nombre de classes optimales.

On a trouvé cette méthode plutôt graphique peu précise informatiquement. Il a donc été décidé d’essayer d’appliquer une métrique “silhouette ”\cite{ref}. Celle-ci consiste à calculer la différence entre la distance moyenne d’un point x avec tous les points de son cluster $a(x)=\frac{1}{|C_{k}|-1}\sum_{u \in C_{k},u\neq x}d(u,x)$, et la plus petite valeur que pourrait prendre a(x) si x appartenait à un autre cluster $b(x)=\min_{l\neq k} \frac{1}{|C_{l}|}\sum_{u \in C_{l}}d(u,x)$. On la normalise par le max afin d'avoir une valeur entre -1 et 1 :

\[s(x)=\frac{b(x)-a(x)}{max(a(x),b(x))}\] où \[a(x)=\frac{1}{|C_{k}|-1}\sum_{u \in C_{k},u\neq x}d(u,x)\]  et \[b(x)=\min_{l\neq k} \frac{1}{|C_{l}|}\sum_{u \in C_{l}}d(u,x)\]

Il faut donc calculer cette métrique pour tous les points et prendre la moyenne. On remarque que celle-ci appartient à [-1,1]. Pour un point, plus la métrique est proche de 1 et plus l'assignation du point à son cluster est satisfaisante. On en déduit donc que plus la silhouette sera proche de 1, et plus le clustering sera bon. 

Finalement, on s’est rendu compte que la méthode silhouette était strictement croissante pour beaucoup trop de k et comportait donc peu d’intérêt pour nous. Du coup, on s’est rabattu sur la méthode du coude en utilisant comme critère les pentes afin de chercher à trouver le k présent dans le coude. Comme les points ne forment pas toujours un "coude" parfait et ne sont pas forcément décroissants, on a utilisé une régression polynômiale d'un ordre assez élevé afin d'approximer la courbe. On choisit alors un critère comme par exemple avoir une pente inférieur à 10\% de la première pente.

\begin{figure}[h]
 \includegraphics[scale=0.5]{./DistorsionGMM.png}
 \includegraphics[scale=0.5]{./DistorsionGMMafterplynomialregression.png}
\end{figure}

\newpage

\subsection{Covariance et contraste}
\begin{itemize}
\item Après avoir appliqué le premier k-means sur les données, et avoir eu un premier clustering, on ne garde qu’un certain pourcentage (20\%) des points les plus proches de chaque centre de cluster dans le but d’éliminer les points mal classifiés.

\item On calcule ensuite les matrices de covariance des variables aléatoires suivant des lois uniformes sur l’ensemble des points de chaque cluster. On définit la distance d’un point à un cluster de la sorte : soit $C^{-1}$ l’inverse de la matrice de covariance du cluster et P son centre. $d=\|C^{-1}(M-P)\|$ est la distance normalisée du point M au cluster. Pour chaque point qui n’est pas encore dans un cluster, on cherche quel est le cluster le plus proche selon ces distances et on l’associe à notre point (sans modifier la matrice de covariance du cluster). On utilise ainsi la distance en nombre d’écarts-types.

\subsubsection*{Démonstration}

Normalisation : \\ On note C la matrice de covariance du vecteur aléatoire \mbox{$(X_{i})_{i \in [\![1;n]\!] }$}. 
C est symétrique donc d'après le théorème spectral, C est diagonalisable en base orthonormée, et ses valeurs propres sont les écarts-types \mbox{$\sigma_{i}$}. Donc $C=PDP^{-1}$ où $D=(\sigma_{i})_{i \in [\![1;n]\!]}$.
\[\|C^{-1}M\| = \|P^{-1}D^{-1}PM\|= \|D^{-1}M'\|= \sum \frac{1}{\sigma_{i^2}}{M'_{i^2}}\]

\item Pour effectuer le contraste, on calcule la différence de chaque donnée avec le centre du cluster.

\item On effectue une projection non linéaire appelée sharpening (provisoirement, on ne garde que les 10\% des plus grandes composantes de chaque vecteur).

\item On applique l’algorithme de k-means puis les deux premières étapes de ce paragraphe à ces données.

\end{itemize}

\subsubsection*{Résultat}

Les clusters ne sont pas vraiment réalistes. Le problème vient de la faible proportion de points gardés par cluster(20\%) et du fait que les gros clusters ont tendance à absorber tous les points en raison de leur écart-type supérieur.
On tentait d’utiliser un critère pertinent pour un seul cluster afin de comparer les éléments de deux clusters distincts. Or si un cluster déterminé par k-means est beaucoup plus diffus que les autres, on constate expérimentalement qu’il absorbe énormément de points lors de la phase de reconstruction avec le contraste.

\begin{figure}[h]
 \begin{center}
  \includegraphics[scale=0.25]{./Problemedenormalisation.png}
 \end{center}
 \caption{Illustration du problème de variance}
\end{figure}


\section{Second programme}


\paragraph{}

L'idée d'utiliser la variance pour normaliser les distances n'ayant pas abouti, nous avons décidé de sauter l'étape de sélection et de faire confiance à l'algorithme de clustering. De plus, pour améliorer le modèle, nous avons choisi d'utiliser l'algorithme Gaussian Mixture Model (GMM) qui fonctionne de la même manière que k-means mais qui tend à chercher des clusters gaussiens. De même que pour k-means, GMM nécessite de connaître le nombre de cluters, ou alors de l'estimer avec la méthode du coude par exemple.

Pour le contraste, nous avons dans un premier temps envisagé de ne garder que la dimension la plus particulière, c'est-à-dire la dimension telle que le contraste soit maximal. L'utilisation du sharpening avec un seuil à paramétrer interviendra dans un second temps. 

\begin{figure}[h]
	\includegraphics[scale=0.5]{./Classification_Cancer.png}
    \includegraphics[scale=0.5]{./Sharpening.png}
\end{figure}

Pour tester notre programme, il est nécessaire de pouvoir vérifier manuellement que les clusters de catégorie et de contraste sont cohérents et que les éléments extérieurs sont rattachés aux bons clusters. Pour cela, nous avons crée une base de données avec des labels caractérisant les fruits. Par exemple, un fruit du type abricot et avec une grande longueur, une petite largeur et un coefficient rouge élevé sera qualifié de : abricot grand fin rouge. Pour labelliser les clusters, on choisit le label majoritaire.

\begin{figure}[h]
    \includegraphics[scale=0.5]{./Centre_b_nine.png}
    \includegraphics[scale=0.5]{./Centre_maligne.png}
\end{figure}

\begin{figure}[h]
 \begin{center}
  \includegraphics[scale=0.35]{./schemas_mercredi.jpg}
 \end{center}
 \caption{Diagramme d'activité}
\end{figure}

\subsubsection*{Résultat}

Les résultats de l'algorithme étaient plutôt satisfaisants. Par exemple, lorsqu'on donnait en entrée un abricot avec une longueur aberrante, il renvoyait la bonne catégorie (ici abricot) ainsi que le label "longueur" qui était atypique. 

\newpage

\section{Troisième programme}

Dans un second temps, nous avons appliqué le second clustering à tous les vecteurs de contraste en appliquant le sharpening à hauteur de 10\% du contraste maximal. Plus précisément, le programme retourne les caractéristiques particulières trouvées grâce au contraste.

Sur un exemple, en donnant en entrée un abricot avec une longueur et un taux de sucre élevés, on reçoit la catégorie abricot puis "longueur +" et "sucre +" pour traduire le fait que la valeur est atypique.


Nous avons développé une interface graphique pour visualiser plus facilement les résultats. L'interface permet d'entrer des valeurs sur chaque dimension puis de connnaître la catégorie et le contraste de l'objet décrit. De plus, le programme affiche un graphe avec deux dimensions choisies en abscisse et en ordonnée. 


\begin{figure}[h]
	\begin{center}
    	\includegraphics[scale=0.25]{./ig.png}
	\end{center}
    \caption{Interface graphique}
\end{figure}

\newpage

\section{Quatrième programme}

Dans cette version du programme, on va gérer les infinis.

Tout d'abord, on fait une étape alternative entre le GMM et le contraste. On cherche à garder les points assez proches du centre du cluster. Pour cela, on fixe par un seuil que l'on nommera infini et qui sera par exemple de 3 écarts-types. On crée un nouveau cluster appelé "core" qui contiendra les points les plus au centre du cluster.

On définit une nouvelle donnée appelée "prototype" qui va contenir les centres des core ainsi que les écarts-types selon chaque dimension.

Dans la partie contraste, nous avons décidé d'effectuer le sharpening par ligne. En effet, les valeurs étant normalisées, cela permet de ne garder que les dimensions les plus importantes de chaque objet. De plus, on effectue ce contraste sur les points du core.

Pour la normalisation, on peut avoir des cas où les écarts-types sont nuls et donc la valeur infinie ce qui est problématique pour effectuer le second GMM sur le contraste. Lorsqu'un écart type est nul, on a alors deux cas :
\begin{itemize}
 \item soit la valeur d'un objet sur cette dimension est nulle et alors la valeur normalisée doit aussi être nulle (le point a la même valeur que le centre sur cette dimension)
 \item soit la valeur d'un objet sur cette dimension est non nulle et alors la valeur normalisée va valoir l'infini. Il faut alors mettre cet objet à part.
\end{itemize}


\section{Méthode incrémentale}

Pour tenter de régler le problème des petits jeux de données qui ne permettent pas d’obtenir des clusters satisfaisants, l’idée de la méthode incrémentale est de créer ou de compléter les clusters au fur et à mesure de l’obtention des données. Nous nous inspirâmes de la biologie : de la même manière que les espèces animales et végétales peuvent être classifiées sous forme d'arbre, correspondant à l'évolution darwinienne, il paraît raisonnable que des données quelqonques puissent appartenir à une hiérarchie de catégories, qui forme un arbre. Cet arbre est formé de noeuds, qui correspondent à des catégories, imbriquées les unes dans les autres, et de feuilles, qui représetent des données.

\paragraph{}

L'ajout d'une nouvelle donnée à l'arbre se fait de manière récursive. Supposons que cette donnée corresponde à la description d'un chat, il faut d'abord déterminer si, parmitous les êtres vivants, cette donnée est plutôt un vertébré, ou un invertébré. Une fois qu'il sera identifié comme vertébré, il faudra le classifier parmi les poissons, les oiseaux, etc. Ainsi, en partant de la racine, le chat subit une suite de classifications successives, qui le font descendre dans l'arbre, jusqu'à ce qu'il rejoigne les autres chats, dans un éventuel sous-cluster constitué de chats.

\paragraph{}

Pour comparer les éléments entre eux, la première idée a été d’utiliser la distance euclidienne et la variance dans l’esprit de la première méthode. Cependant, il nous parût plus pertinent de nous inspirer de la théorie de la complexité, en disant qu'une donnée est proche d'une autre si elle a un grand nombre de dimensions en commum. Par exemple un tigre et un chat diffèrent par leur taille, mais sont identiques sur les dimensions "nombre de pattes", "nombre de griffes", etc.

\paragraph{}

L'un des avantages de cette approche est la possibilité d'un apprentissage amnésique : les données les plus communes pourraient être oubliées, au profit des seuls clusters, tandis que les données plus exceptionnelles seraient conservées. Pour discriminer les données utiles à conserver, un calcul de complexité pourrait être utilisé. Celle-ci peut être estimée en sommant les complexités du chemin dans l'arborescence, et celles correspondant au rang de la donnée parmi les données soeurs. 

\subsubsection*{Résultat}

\begin{figure}[h]
 \begin{center}
  \includegraphics[scale=0.6]{./arbre.png}
 \end{center}
 \caption{Arbre renvoyé par l'algorithme incrémental}
\end{figure}


\medskip

\bibliographystyle{unsrt}
\bibliography{biblio}

\appendix %annexes

\end{document}
